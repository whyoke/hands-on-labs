{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your first Image-to-text with Gradio and LLaVA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the documentation at https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# This code would take a while to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While running this code, you can learn about LLaVa model from here\n",
    "[LLaVa](https://llava-vl.github.io/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ilankelman.org/stopsigns/australia.jpg\" ## click on the link to see the image\n",
    "# or this image \n",
    "# url = \"https://llava-vl.github.io/static/images/monalisa.jpg\"\n",
    "\n",
    "image_stop = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "## Display the image\n",
    "image_stop.show()\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"}, # This is the image input\n",
    "            {\"type\": \"text\", \"text\": \"What is shown in this image?\"}, # This is the text input\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create prompt from conversation (image and text)\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True) \n",
    "\n",
    "# Process the image and prompt\n",
    "inputs = processor(\n",
    "    images=[image_stop], \n",
    "    text=[prompt], \n",
    "    return_tensors=\"pt\"\n",
    ").to(device=\"cuda\", dtype=torch.float16) # Send the inputs to the GPU\n",
    "\n",
    "\n",
    "generate_ids = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "output_text = processor.batch_decode(generate_ids, skip_special_tokens=True) # Decode the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_text)\n",
    "\n",
    "## The output text is contain \"USER\" input and the generated text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter the output text to get the answer after \"ASSISTANT:\"\n",
    "answer = output_text[0].split(\"ASSISTANT:\")[1].strip()\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put everything into one function and then test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(image, prompt = \"What is shown in this image?\", max_new_tokens=200):\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        images=[image],\n",
    "        text=[prompt],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device=\"cuda\", dtype=torch.float16)\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    generated_description = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "\n",
    "    return generated_description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then serve using Gradio. `input` will be images and textbox (prompt) and output will be text (description of the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function that we just build\n",
    "image = Image.open(\"/content/466029110_1113670126421850_13431688209473903_n.jpg\")\n",
    "\n",
    "generate_description(\n",
    "    image,\n",
    "    \"What is shown in this image?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The output text is contain \"USER\" input and the generated text from the model\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=lambda img, prompt: generate_description(img, prompt),\n",
    "    inputs=[gr.Image(type=\"pil\"),\n",
    "            gr.Textbox(label=\"prompt\", value=\"What is shown in this image?\", lines=3)],  # Changed to numpy\n",
    "    outputs=[gr.Textbox(label=\"Description\", lines=3)],\n",
    "    title=\"Image Description using LLaVA\",\n",
    "    description=\"Upload an image to get a detailed description using LLaVA-1.5-7b\",\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can leave a lot of port open. So don't forget to close all the port using `gr.close_all()`\n",
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
